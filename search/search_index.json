{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Wannabe-DevOps","text":"<p>Here you will find random guides and projects that I'm interested in documenting for educational purposes.</p>"},{"location":"#about-me","title":"About me","text":"<p>I'm Daniel. I come from Sysadmin background and now making my transition to DevOps \ud83d\ude0e</p>"},{"location":"AWS/AWS%20SSM%20Patch%20Group/","title":"AWS SSM Patch Group","text":"<p>Recently I needed to add multiple instances to a Patch Group. - This is done by adding a tag to each instances where the key is Patch Group and the value is a name of your choice. If you use AWS\u2019s SSM service, you know that through the Console you are only able to add one tag at a time.</p> <p>This short script will show you how to tag multiple instances at once. Before that, we need to differentiate between EC2 instances and SSM Managed Instances.</p> <ul> <li>EC2 Instance \u2013 a regular EC2 instance. Represented by an instance ID that starts with i-</li> <li>Managed Instance \u2013 An on-premise or non EC2 managed instance that you can manage in SSM. Represented by and instance ID that starts with mi-</li> </ul> <p>Before you start, make sure you have 2 things:</p> <ul> <li>Boto3 installed</li> <li>The appropriate IAM permissions (use aws configure to set your test environment)</li> </ul> <p>I want to add all of my Amazon Linux 2 machines to their own Patch Group. In my example, I only add the tags to EC2 instances, but the same logic applies to managed instances as well.</p> <pre><code>import boto3\nssm_client = boto3.client('ssm')\nec2_client = boto3.client('ec2')\n\ntags = {'Key': 'Patch Group',\n        'Value': 'AL2-Test'\n}\n\nall_instances = []\nfor instance in ssm_client.describe_instance_information()['InstanceInformationList']:\n        if instance['PlatformName'] == 'Amazon Linux' and instance['PlatformVersion'] == '2':\n                all_instances.append(instance['InstanceId'])\n\nfor instance in all_instances:\n        ec2_client.create_tags(\n                # DryRun=True,\n                Resources=[instance],\n                Tags=[tags]\n        )\n</code></pre> You can choose whatever tags you want for your key/value <pre><code>tags = {'Key': 'Patch Group',\n    'Value': 'AL2-Test'}\n</code></pre> <p>You can also use more tags.</p> <p>The first loop iterates over all of the instances in SSM, and if the instance\u2019s OS is Amazon Linux 2, it gets added to a list of instances.</p> <p>The second loop goes over the list of instances we just filled and simply creates the tags we configured. As you can see, by running we get our desired result:</p> <p></p> <p>The same logic can be applied to SSM Managed Instances. Instead of using the EC2 boto3 client, we can utilize the same SSM client we already have and use the add_tags_to_resource function: <pre><code>response = client.add_tags_to_resource(\n    ResourceType='Document'|'ManagedInstance'|'MaintenanceWindow'|'Parameter'|'PatchBaseline'|'OpsItem'|'OpsMetadata',\n    ResourceId='string',\n    Tags=[\n        {\n            'Key': 'string',\n            'Value': 'string'\n        },\n    ]\n)\n</code></pre></p> <p>That\u2019s it. Hope it has been helpful to someone \ud83d\ude0a</p>"},{"location":"Argo%20Workflows/How%20to%20debug%20workflows/","title":"Easily Debug Argo Workflows","text":"<p>Argo Workflows has a great feature that allows you to pause a certain step, and since each step runs in its own pod, it makes it simple to simply connect to that pod to troubleshoot. Let's see how it works.</p>"},{"location":"Argo%20Workflows/How%20to%20debug%20workflows/#how-to","title":"How to","text":"<p>Argo's documentation shows us how to do this. All we have to do is add the following environment variable to our step: <pre><code>env:\n  - name: ARGO_DEBUG_PAUSE_AFTER\n    value: 'true'\n</code></pre> You can also use <code>BEFORE</code>: <pre><code>env:\n  - name: ARGO_DEBUG_PAUSE_BEFORE\n    value: 'true'\n</code></pre> The differences are self-explanatory, where the former would pause the step after it was executed, and the latter would pause it before.</p>"},{"location":"Argo%20Workflows/How%20to%20debug%20workflows/#example","title":"Example","text":"<p>Let's use this simple workflow: <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: my-run-\nspec:\n  serviceAccountName: argo-workflows-server\n  entrypoint: main\n  templates:\n    - name: main\n      steps:\n        - - name: step-1\n            template: print-arg-tmpl\n            arguments:\n              parameters:\n                - name: arg\n                  value: \"this is step 1\"\n\n        - - name: step-2\n            template: print-arg-tmpl\n            arguments:\n              parameters:\n                - name: arg\n                  value: \"this is step 2\"\n\n    - name: print-arg-tmpl\n      inputs:\n        parameters:\n          - name: arg\n      script:\n        image: bash\n        command: [bash]\n        source: |\n          echo \"{{inputs.parameters.arg}}\"\n</code></pre></p> <p>All this workflow does is print stuff that we pass to the template. Let's run it: <code>argo submit argo-wf/example.yml</code> </p> <p>As we can see, the workflow finished with no errors and took 20 seconds to run: </p> <p>At this point, let's edit out template and add a Debug pause: <pre><code>- name: print-arg-tmpl\n  inputs:\n    parameters:\n      - name: arg\n  script:\n    image: bash\n    command: [bash]\n    source: |\n      echo \"{{inputs.parameters.arg}}\"\n    env:\n      - name: ARGO_DEBUG_PAUSE_AFTER\n        value: 'true'\n</code></pre> The result is that our <code>step-1</code> is paused:  We can even check the log to see that indeed ran before pausing, since we used <code>ARGO_DEBUG_PAUSE_AFTER</code> and not <code>BEFORE</code>:</p> <p></p> <p>We can now connect to the pod and start troubleshoothing  Run <code>kubectl exec -it &lt;pod-name&gt; -- bash</code> and you should have access to the pod:</p> <p></p>"},{"location":"Argo%20Workflows/How%20to%20debug%20workflows/#exit-debug-mode","title":"Exit Debug Mode","text":"<p>Once you're finished, you can simply type <code>touch /proc/1/root/var/run/argo/ctr/main/after</code> inside of the terminal, it will signal the pod to exit the debug mode and continue to the next steps:  In our case, since we're using the same template for <code>step-2</code>, it will pause as well:  I'll exit that one as well and the workflow will finish successfully</p> <p></p> <p>Note</p> <p>If you used <code>ARGO_DEBUG_PAUSE_BEFORE</code>, the command to exit debug mode changes from <code>after</code> to <code>before</code>: <pre><code>touch /proc/1/root/var/run/argo/ctr/main/before\n</code></pre></p>"},{"location":"Argo%20Workflows/How%20to%20debug%20workflows/#bonus","title":"Bonus","text":"<p>Something I personally really like is the Kubernetes extension in VS-Code, which allows you to attach a VS-Code instance to the pod you're debugging:  Right click on your pod and open it in VS Code: </p> <p>That's it, now you can even debug code if you need to. It's very helpful when you're working with pods that pull images with your code. For example, you can change code in-real time inside of the pod.</p> <p>That's pretty much it, I love this feature \ud83d\ude0e</p>"},{"location":"ArgoCD/Setup%20Local%20Dev%20Environment/","title":"Setup ArgoCD Locally","text":"<p>In many cases you find yourself testing things in local environments, whether it's scripts or workflows, you need a safe space to break things and learn.</p> <p>ArgoCD is no exception, and while ArgoCD is native to Kubernetes, you don't need to spend money by running clusters in some cloud. I'll be using KinD to create a cluster on my laptop and install everything we need.</p>"},{"location":"ArgoCD/Setup%20Local%20Dev%20Environment/#introduction","title":"Introduction","text":"<p>We will use 2 repos: - Orangutan-infra - Will manage the infrastructure. - Orangutan - Will manage the configuration files.</p>"},{"location":"ArgoCD/Setup%20Local%20Dev%20Environment/#installation","title":"Installation","text":""},{"location":"ArgoCD/Setup%20Local%20Dev%20Environment/#create-kubernetes-cluster","title":"Create Kubernetes Cluster","text":"<ol> <li>Create a cluster: <code>kind create cluster --name playground</code>  You can run <code>kubectl get nodes</code> to make sure the control-plane is running.</li> </ol>"},{"location":"ArgoCD/Setup%20Local%20Dev%20Environment/#preparing-the-files","title":"Preparing The Files","text":"<p>We'll use Terraform to install ArgoCD. We'll utilize Terraform's Helm provider to install ArgoCD instead of directly applying the chart on our cluster.</p> <p>Before installing the Helm chart, you can run <code>helm search repo argocd</code> to search for the chart: </p> <p>Note</p> <p>You can run <code>helm show values argo/argo-cd &gt; argo-default-values.yaml</code> to export the default values of the chart. It's helpful when you want to see what values exist for each chart before deploying it.</p> <ol> <li>In <code>orangutan-infra</code> I'll create the following folders and files: <pre><code>\u2514\u2500\u2500 terraform\n    \u2514\u2500\u2500 argocd\n        \u251c\u2500\u2500 0-provider.tf\n        \u251c\u2500\u2500 1-argocd.tf\n        \u2514\u2500\u2500 values\n            \u2514\u2500\u2500 argocd.yaml\n</code></pre> For our values file <code>argocd.yaml</code>, we'll pass the following: <pre><code>---\nserver:\n  extraArgs:\n    - --insecure\n</code></pre> <code>--insecure</code> will serve our UI on HTTP.</li> </ol>"},{"location":"ArgoCD/Setup%20Local%20Dev%20Environment/#0-providertf","title":"<code>0-provider.tf</code>","text":"<p>We'll configure Helm as our provider and pass <code>~/.kube/config</code> to use the local cluster we created: <pre><code>provider \"helm\" {\n  kubernetes {\n    config_path = \"~/.kube/config\"\n  }\n}\n</code></pre></p>"},{"location":"ArgoCD/Setup%20Local%20Dev%20Environment/#1-argocdtf","title":"<code>1-argocd.tf</code>","text":"<p>For the Helm installation, we'll use the the following block: <pre><code>resource \"helm_release\" \"argocd\" {\n  name = \"argocd\"\n\n  repository       = \"https://argoproj.github.io/argo-helm\"\n  chart            = \"argo-cd\"\n  namespace        = \"argocd\"\n  create_namespace = true\n\n  values = [file(\"values/argocd.yaml\")]\n}\n</code></pre></p>"},{"location":"ArgoCD/Setup%20Local%20Dev%20Environment/#installing-argocd","title":"Installing ArgoCD","text":"<ol> <li>Run <code>terraform init</code> to initialize Terraform. If it was successful, you should see the following output:</li> <li> <p>We're ready to run <code>terraform apply</code>. If we did everything correctly, the output will show us the resources that we're deploying. Enter <code>yes</code> and wait for ArgoCD to finish installing.</p> </li> <li> <p>If all went well, you should be able to see our pods are in <code>Running</code> state:       </p> </li> </ol> <p>Tip</p> <p>You can run <code>helm status argocd -n argocd</code> to follow the deployment status:   </p>"},{"location":"ArgoCD/Setup%20Local%20Dev%20Environment/#accessing-argocd","title":"Accessing ArgoCD","text":"<ol> <li>We need to port forward the Argo Server in order to access it:    </li> <li>Go to <code>http://localhost:8080</code>. You'll need to enter a username and password.    By default, the username is <code>admin</code>.    To get the password, we'll need to first fetch the secret that Argo created in our cluster:    <pre><code>k get secrets argocd-initial-admin-secret -oyaml -n argocd\n</code></pre>     The password is base64 encrypted. Let's decrypt it (don't copy the <code>%</code> char, it just states the end of the string):    </li> <li>Login to Argo's UI and change your password    </li> </ol> <p>That's it, you successfully installed ArgoCD locally \u2705</p>"},{"location":"GitHub%20Actions/Comment%20on%20PR%20from%20Markdown%20file/","title":"Comment on PR using Markdown","text":"<p>We recently implemented a \"message of the day\" type of concept in some of our workflows. Here's how we template a nice message in our PRs:</p> <pre><code>name: PR Welcome Comment\non:\n  pull_request:\n    types: [opened]\n\njobs:\n  comment:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v4\n\n    - name: Read .md file and set output\n      shell: bash\n      run: |\n        echo 'README&lt;&lt;EOF' &gt;&gt; $GITHUB_ENV\n        cat ./test.md &gt;&gt; $GITHUB_ENV\n        echo 'EOF' &gt;&gt; $GITHUB_ENV\n\n    - name: Comment on PR\n      uses: peter-evans/create-or-update-comment@v2\n      with:\n        issue-number: ${{ github.event.pull_request.number }}\n        body: ${{ env.README }}\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n</code></pre> <p>Make sure to change the path <code>./test.md</code> to your markdown file. The result: </p> <p>Note</p> <p>You might want to reference the <code>.md</code> file from another repo (i.e your reusable repo). Check out this guide.</p> <p>That's it, pretty fun option to have in your environment \ud83e\udee1</p>"},{"location":"GitHub%20Actions/Custom%20GitHub%20Env%20Variables%20From%20Reusable%20Workflow/","title":"How To Use Custom Environment Variables From Reusable Workflow","text":"<p>Let's go over how we can use custom environment variables when working with reusable workflow. This is specifically helpful when you want to pass env variables to your caller workflow without adding more and more parameters to your workflow.</p>"},{"location":"GitHub%20Actions/Custom%20GitHub%20Env%20Variables%20From%20Reusable%20Workflow/#background","title":"Background","text":"<p>When we want to pass parameters between reusable workflows, you'd typically create an input for each variable that you want, for example: <pre><code>name: Workflow\non:\n  workflow_call:\n    inputs:\n      var1:\n        required: false\n        type: string\n      var2:\n        required: true\n        type: string\n</code></pre></p> <p>You would then reference these parameters in your caller workflow, for example: <pre><code>- uses: some-action\n  with:\n    var1: ${{ inputs.var1 }}\n    var2: ${{ inputs.var2 }}\n</code></pre> When you want to pass multiple vars, you would then have to create more inputs. This can cause you to chase after other caller workflows and adjust their existing configuration which in the end is kind of tedious work.</p>"},{"location":"GitHub%20Actions/Custom%20GitHub%20Env%20Variables%20From%20Reusable%20Workflow/#alernative","title":"Alernative","text":"<p>We can consolidate the custom environment variables into a single parameter, like so:  <pre><code>name: Reusable - Environment Variables\non:\n  workflow_call:\n    inputs:\n      env_vars:\n        description: \"Environment variable\"\n        required: false\n        type: string\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Parse and set environment variables\n        run: |\n          env_vars=(${{ inputs.env_vars }})\n          for var in ${env_vars[*]}; do\n            echo $var &gt;&gt; \"$GITHUB_ENV\"\n          done\n\n      - name: Print environment variables\n        run: |\n          env\n</code></pre></p> <p>Then, in the caller workflow we can use the following step: <pre><code>name: Caller - Environment Variables\n\non:\n  workflow_dispatch:\n\njobs:\n  call-reusable-workflow:\n    name: call-reusable-workflow\n    uses: dbeilin/PrimateCI/.github/workflows/env_reusable.yml@main\n    with:\n      env_vars: |\n        foo=bar\n        waldo=fred\n</code></pre></p>"},{"location":"GitHub%20Actions/Custom%20GitHub%20Env%20Variables%20From%20Reusable%20Workflow/#result","title":"Result","text":"<p>Let's call the above workflows:</p> <p></p> <p></p> <p>The workflow finished successfully:</p> <p></p> <p>If we look through all of the environment variables, we'll find our custom ones:</p> <p></p> <p>That's it. I believe this is a pretty neat trick that can be useful some for use-cases \ud83d\ude0e</p>"},{"location":"GitHub%20Actions/Reference%20Files%20From%20Reusable%20Repo/","title":"Use Files From Another Repo","text":""},{"location":"GitHub%20Actions/Reference%20Files%20From%20Reusable%20Repo/#background","title":"Background","text":"<p>While I was trying to implement a workflow that templates an automatic PR comment from a Markdown file, I stumbled upon an issue where I couldn't call my <code>.md</code> file which was supposed to sit in anothr repo. We have a dedicated repository that holds all of our reusable Github Workflows, which we then reference in each of our services as needed. If I simply put my Markdown file in the same repo as our reusable workflows, the file would not be found when some repo calls this workflow.</p>"},{"location":"GitHub%20Actions/Reference%20Files%20From%20Reusable%20Repo/#example","title":"Example","text":"<p>The following would only work if <code>test.md</code> was located in the caller repo: <pre><code>name: PR Welcome Comment\non:\n  workflow_call:\n\njobs:\n  comment:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v4\n\n    - name: Read .md file and set output\n      shell: bash\n      run: |\n        echo 'README&lt;&lt;EOF' &gt;&gt; $GITHUB_ENV\n        cat ./test.md &gt;&gt; $GITHUB_ENV\n        echo 'EOF' &gt;&gt; $GITHUB_ENV\n\n    - name: Comment on PR\n      uses: peter-evans/create-or-update-comment@v2\n      with:\n        issue-number: ${{ github.event.pull_request.number }}\n        body: ${{ env.README }}\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n</code></pre></p>"},{"location":"GitHub%20Actions/Reference%20Files%20From%20Reusable%20Repo/#workaround","title":"Workaround","text":"<p>In the reusable workflow, we can create 2 files:</p> <ul> <li>Composite</li> <li>Github Action</li> </ul> <p>Create the composite workflow using the same logic: <pre><code>name: pr-comment\ndescription: pr-comment\n\nruns:\n  using: \"composite\"\n  steps:\n    - name: Get markdown file\n      shell: bash\n      run: |\n        echo 'README&lt;&lt;EOF' &gt;&gt; $GITHUB_ENV\n        cat ${{ github.action_path }}/test.md &gt;&gt; $GITHUB_ENV\n        echo 'EOF' &gt;&gt; $GITHUB_ENV\n</code></pre></p> <p>And then call it like so in the reusable workflow: <pre><code>name: PR Welcome Comment\non:\n  workflow_call:\n\njobs:\n  comment:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Fetch Markdown\n      uses: dbeilin/PrimateCI/composite/common/pr-comment@main\n\n    - name: Comment on PR\n      uses: peter-evans/create-or-update-comment@v2\n      with:\n        issue-number: ${{ github.event.pull_request.number }}\n        body: ${{ env.README }}\n</code></pre></p> <p>Note</p> <p>Both files should sit inside the \"reusable\" repo.</p>"},{"location":"GitHub%20Actions/Reference%20Files%20From%20Reusable%20Repo/#calling-the-workflow","title":"Calling the Workflow","text":"<p>You can now call the workflow like so in any of your repos: <pre><code>name: common-trigger\n\non:\n  pull_request:\n    branches: [main]\n    types: [opened, edited, synchronize, reopened]\n\njobs:\n  pr-comment:\n    name: pr-comment\n    uses: dbeilin/PrimateCI/.github/workflows/pr.yml@main\n    if: ${{ github.event_name == 'pull_request' &amp;&amp; github.event.action == 'opened' }}\n</code></pre></p> <p>This way, you don't have to keep a <code>.md</code> file in your regular repos and it can sit along your reusable workflows. That's it \ud83d\ude0e</p>"},{"location":"Home-Server/Homelab%20Media%20Server/","title":"Homelab Media Server","text":""},{"location":"Home-Server/Homelab%20Media%20Server/#intro","title":"Intro","text":"<p>Hello everyone. In this tutorial I wanted to share my setup for my home automation.</p> <p>It took me a while to get everything going and working properly, but I'm currently at a point where everything is working and I'm happy with the result, so I wanted to share it with you \ud83d\ude0a</p>"},{"location":"Home-Server/Homelab%20Media%20Server/#my-setup","title":"My setup","text":"<ul> <li>HP EliteBook G2 as my home server (running Ubuntu)</li> <li>Asustor AS1004t v2 NAS for my storage (RAID5, 4x1TB HDDs)</li> <li>My ISP's router.</li> </ul>"},{"location":"Home-Server/Homelab%20Media%20Server/#goal","title":"Goal","text":"<ul> <li> Automatically download my media</li> <li> Automatically rename the media and organize in folders</li> <li> Run everything from Dockers</li> </ul>"},{"location":"Home-Server/Homelab%20Media%20Server/#copyright-notice","title":"Copyright Notice","text":"<p>I will be using usenet and torrents to download shows, movies and books. I am not responsible for whatever you'll do with this setup. If piracy is illegal in your country and you're afraid of actions that can be taken by your ISP, I suggest you won't go through with it.</p> <p>With that being said, you can also mask your traffic behind a VPN of your choice, which I won't be covering in this tutorial.</p>"},{"location":"Home-Server/Homelab%20Media%20Server/#apps-well-use","title":"Apps we'll use","text":"<ul> <li>Radarr - managing movies</li> <li>Sonarr - managing tv shows</li> <li>Plex - my media player of choice. A lot of people would also recommend Jellyfin which is open-source and great as well \ud83d\ude0a</li> <li>SABnzbd - my binary newsgroup downloader of choice.</li> <li>qBitTorrent - torrent downloader of choice.</li> </ul>"},{"location":"Home-Server/Homelab%20Media%20Server/#how-to","title":"How-To","text":""},{"location":"Home-Server/Homelab%20Media%20Server/#docker-docker-compose","title":"Docker &amp; Docker-Compose","text":"<p>I will be sharing my docker-compose file which will include all of the relevant apps for this project. You will need to install both Docker and Docker Compose on your server of choice.</p> <p>Installing Docker and Docker-Compose is as simple as it gets: - Docker - Docker-Compose</p>"},{"location":"Home-Server/Homelab%20Media%20Server/#nas-setup-and-paths","title":"NAS Setup and paths","text":"<ol> <li> <p>Create a volume in your NAS. In my case, I created a RAID5 volume with 4 disks     </p> <p>Info</p> <ul> <li>The mount command can vary between systems.</li> <li>Don't forget to change the server's IP to yours.</li> <li>If you didn't name your folder <code>data</code>, change that to whatever you named your folder.</li> </ul> </li> <li> <p>Go to your server and mount the network path.</p> <ol> <li>run <code>mount -o v3 192.168.1.14:/volume1/data /mnt/data</code></li> <li>edit <code>/etc/fstab</code> to have the drive mount itself on boot: <code>192.168.1.14:/volume1/data /mnt/data nfs defaults 0 0</code></li> <li>run <code>mount --all</code></li> </ol> </li> <li> <p>You should now see your drive mounted: <pre><code>\u276f df -h\nFilesystem                  Size  Used Avail Use% Mounted on\n/dev/sda5                   234G   23G  200G  11% /\n192.168.1.14:/volume1/data  2.7T  1.6T  1.2T  57% /mnt/data\n...\n</code></pre></p> <p>Tip</p> <p>From here we will be doing exactly as stated under TRaSH guides. Please more about it if you wish to understand why.</p> </li> <li> <p>Open your mount, and create folders using the following structure: <pre><code>data\n\u251c\u2500\u2500 torrents\n\u2502  \u251c\u2500\u2500 movies\n\u2502  \u2514\u2500\u2500 tv\n\u251c\u2500\u2500 usenet\n\u2502  \u251c\u2500\u2500 movies\n\u2502  \u2514\u2500\u2500 tv\n\u2514\u2500\u2500 media\n   \u251c\u2500\u2500 movies\n   \u2514\u2500\u2500 tv\n</code></pre></p> </li> <li> <p>In the ends, once you go into <code>/mnt/data</code>, the folder should look like this: <pre><code>beinish in Media-Server in /mnt/data\n\u276f ls -l\ntotal 16\ndrwxrwxr-x 5 beinish          beinish          4096 Oct  4 14:04  media\ndrwxrwxr-x 5 beinish          beinish          4096 Oct  4 14:02  torrents\ndrwxrwxr-x 6 beinish          beinish          4096 Oct 19 15:36  usenet\n</code></pre></p> </li> </ol>"},{"location":"Home-Server/Homelab%20Media%20Server/#portainer","title":"Portainer","text":"<p>I chose Portainer to avoid managing my Docker containers via CLI. Installing Portainer is very simple, you can choose the method of installation from their website.</p> <p>After installing it, make sure it's running: <pre><code>root@server:~# docker ps\nCONTAINER ID   IMAGE                          COMMAND                  CREATED       STATUS      PORTS                                                                                  NAMES             \nde5b28eb2fa9   portainer/portainer-ce:2.9.3   \"/portainer\"             2 weeks ago   Up 9 days   0.0.0.0:8000-&gt;8000/tcp, :::8000-&gt;8000/tcp, 0.0.0.0:9443-&gt;9443/tcp, :::9443-&gt;9443/tcp   portainer\n</code></pre></p>"},{"location":"Home-Server/Homelab%20Media%20Server/#compose-file","title":"Compose-File","text":"<p>The following docker-compose file is configured exactly as it should, we will make some changes after running it:</p> <p>Information</p> <ul> <li><code>PUID</code> - Usually you can leave this on <code>1000</code></li> <li><code>PGID</code> - Same as <code>PUID</code>.</li> <li>You can check double check by running: <code>id $user</code></li> <li>Under <code>volumes</code>, change <code>/mnt/data:/data</code> to reflect your setup. If you mounted <code>/mnt/data</code> like me, you can leave it as is.</li> <li>Repeat the above step for each container that has it!</li> <li>Change <code>TZ=</code> to reflect your timezone. You can find the value for your location here.</li> <li>It doesn't matter too much where to store the config files for each container, I personally do <code>/docker/appdata/{app}:/config</code>. Instead of <code>{app}</code>, I change it to the name of the app/container I'm using.</li> </ul> <pre><code>version: \"3.2\"\nservices:\n  radarr:\n    container_name: radarr\n    image: cr.hotio.dev/hotio/radarr:latest\n    restart: unless-stopped\n    logging:\n      driver: json-file\n    ports:\n      - 7878:7878\n    hostname: radarr\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - TZ=Asia/Jerusalem\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - /docker/appdata/radarr:/config\n      - /mnt/data:/data\n\n  sonarr:\n    container_name: sonarr\n    image: cr.hotio.dev/hotio/sonarr:latest\n    restart: unless-stopped\n    logging:\n      driver: json-file\n    ports:\n      - 8989:8989\n    hostname: sonarr\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - TZ=Asia/Jerusalem\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - /docker/appdata/sonarr:/config\n      - /mnt/data:/data\n\n  bazarr:\n    container_name: bazarr\n    image: cr.hotio.dev/hotio/bazarr:latest\n    restart: unless-stopped\n    logging:\n      driver: json-file\n    ports:\n      - 6767:6767\n    hostname: bazarr\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - TZ=Asia/Jerusalem\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - /docker/appdata/bazarr:/config\n      - /mnt/data/media:/data/media\n\n  qbittorrent:\n    container_name: qbittorrent\n    image: cr.hotio.dev/hotio/qbittorrent\n    ports:\n      - \"8080:8080\"\n    hostname: qbittorrent\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - UMASK=002\n      - TZ=Asia/Jerusalem\n    volumes:\n      - /docker/appdata/qbittorrent:/config\n      - /mnt/data/torrents:/data/torrents\n\n  plex:\n    container_name: plex\n    image: cr.hotio.dev/hotio/plex\n    # ports:\n    #   - \"32400:32400\"\n    hostname: plex\n    network_mode: host\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - UMASK=002\n      - TZ=Asia/Jerusalem\n      - PLEX_CLAIM\n      - ADVERTISE_IP\n      - ALLOWED_NETWORKS\n      - PLEX_PASS=no\n    volumes:\n      - /docker/appdata/plex:/config\n      - /docker/appdata/plex/transcode:/transcode\n      - /mnt/data/media:/data/media\n\n  sabnzbd:\n    image: lscr.io/linuxserver/sabnzbd:latest\n    container_name: sabnzbd\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - TZ=Asia/Jerusalem\n    volumes:\n      - /docker/appdata:/config\n      - /mnt/data/usenet:/data/usenet\n    ports:\n      - 8081:8081\n    hostname: sabnzbd\n    restart: unless-stopped\n\n  jackett:\n    container_name: jackett\n    image: cr.hotio.dev/hotio/jackett\n    ports:\n      - \"9117:9117\"\n    hostname: jackett\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - UMASK=002\n      - TZ=Asia/Jerusalem\n    volumes:\n      - /docker/appdata/jackett:/config\n\n  overseerr:\n    image: sctx/overseerr:latest\n    container_name: overseerr\n    environment:\n      - LOG_LEVEL=debug\n    ports:\n      - 5055:5055\n    hostname: overseerr\n    volumes:\n      - /docker/appdata/overseerr:/app/config\n    restart: unless-stopped\n</code></pre> <p>Apps in the compose file:</p> <ul> <li>Radarr - Managing movies.</li> <li>Sonarr - Managing TV shows.</li> <li>Bazarr - Managing subtitles for movies and tv shows.</li> <li>Plex - Media player for all of our downloads.</li> <li>qBitTorrent - Torrent downloading client.</li> <li>Sabnzbd - Binary newsreader, our usenet downloader.</li> <li>Jackett - This is our torrent indexer. it essentialy makes queries to torrent sites and retrives the data from them to pass on to radarr/sonarr.</li> <li>Overseerr - Requests manager for our media. Instead of going to any of the *arr apps, we can request media straight from this great UI.</li> </ul>"},{"location":"Home-Server/Homelab%20Media%20Server/#configure-the-apps","title":"Configure the apps","text":"<p>Why?</p> <p>I encourage you to go an read more about Hardlinks to understand why we're configuring how media server this way.</p> <p>We will be configuring the apps based on TRaSH Guides guidelines.</p> <p>For Radarr, Sonarr, Sabnzbd and qBittorrent, make sure you configure it as listed here.</p>"},{"location":"Misc/Automatically%20Set%20Environment%20Variables%20with%20Direnv/","title":"Direnv","text":"<p>If you\u2019re bouncing between multiple AWS accounts or credentials in your CLI, this tool might be a huge quality of life perk for you.</p> <p>Basically, direnv can load and unload environment variables depending on the current directory you\u2019re in. In this post I\u2019ll show how to use it to switch between AWS credentials.</p> <p>Let\u2019s start with a real world example right of the bat: <pre><code>~/playground master* \u276f ls -l\ntotal 20\ndrwxr-xr-x 2 dbei dbei 4096 Apr 21 09:37 project1\ndrwxr-xr-x 2 dbei dbei 4096 Apr 21 09:37 project2\n~/playground master* \n</code></pre></p> <ul> <li>Project1 will use AWS Account #1</li> <li>Project2 will use AWS Account #2</li> </ul> <p>Let\u2019s start with Project1:</p> <ol> <li> <p>Setup direnv.</p> </li> <li> <p>cd into your desired directory. In my case it would be <code>project1</code>.</p> </li> <li> <p>Create .envrc: <code>vi .envrc</code></p> </li> <li> <p>Add your AWS credentials which you can generate in IAM:    <pre><code>export AWS_ACCESS_KEY_ID=AKIAZXIDSFDSNTLBJWC\nexport AWS_DEFAULT_REGION=eu-west-1\nexport AWS_SECRET_ACCESS_KEY=KJuja33rTxIlK/wee/DsdrdhjKKg/HDrmtreDJ\n</code></pre></p> </li> <li> <p><code>cd</code> in and out of that directory.</p> </li> <li> <p>You should see an error: </p> Error <pre><code>direnv: error /playground/project1/.envrc is blocked. Run direnv allow to approve its content\n</code></pre> </li> <li> <p>Type <code>direnv allow</code> and you should be good: <pre><code>~/playground/project1 master* \u276f direnv allow\ndirenv: loading ~/playground/project1/.envrc\ndirenv: export ~AWS_DEFAULT_REGION\n</code></pre></p> </li> <li> <p>You can test that it works with any AWS CLI command. For example: <pre><code>~/playground/project1 master* \u276f aws s3 ls\n2022-03-22 16:35:07 daniel-awesome-bucket\n2022-04-21 09:18:52 do-not-delete-gatedgarden-audit-384466123559\n~/playground/project1 master* \u276f\n</code></pre></p> </li> </ol> <p>Repeat the same process for Project2 and use different credentials. After that, you should be set. From here you can move between folders, which will automatically move between your AWS credentials and accounts!</p> Tip <p>Don\u2019t forget to add <code>.envrc</code> to your <code>.gitignore</code>!</p>"},{"location":"PowerShell/Add%20Multiple%20DNS%20Records/","title":"Add Multiple DNS Records","text":"<p>If you need to add multiple A records to your DNS, here\u2019s a quick way to do so.</p> <p>Create a CSV file, name it however you like (make sure you correct the name of the file in the script itself). Then, the first row should be filled with the words \u201cHostname\u201d and \u201cIP\u201d. You can change that as wel but once again, make sure you edit the script accordingly.</p> <p></p> <p>Then, edit this script with your relevant information like the CSV filename, zone name, and your DC.</p> <pre><code>$IPs = Import-Csv \"C:\\temp\\dnsIPs.csv\"\nforeach ($item in $IPs)\n{\n    $hostname = $item.(\"Hostname\")\n    $IP = $item.(\"IP\")\n    Add-DnsServerResourceRecordA -Name $hostname -ZoneName \"yourcorp.com\" -IPv4Address $IP -ComputerName \"DC1\" -AsJob\n    Get-Job | Stop-Job\n}\n</code></pre> <p>That\u2019s it, now run it and watch it go \ud83d\ude42</p>"},{"location":"PowerShell/Auto-Shutdown%20And%20Delete%20VMs/","title":"Auto Shutdown And Delete VMs","text":"<p>This is a simple script to shutdown and delete multiple VMs at once in VMware.</p> <p>Make sure you have a list of VMs in a text file. It\u2019s simple to export a list of VMs in a certain folder in your vcenter, but just as an example, it should look like this:</p> <p></p> <p>Then, here\u2019s the script, run it:</p> <pre><code>$vcenter = Read-Host(\"Enter VCenter IP\")\n$VMs = (Get-Content C:\\temp\\vmlist.txt)\n$vmObj = Get-vm $vms\nConnect-VIServer $vcenter\n\nforeach($active in $vmObj){\nif($active.PowerState -eq \"PoweredOn\"){\nStop-VM -VM $active -Confirm:$false -RunAsync | Out-Null} \n}\nStart-Sleep -Seconds 7\n\nforeach($delete in $vmObj){\nRemove-VM -VM $delete -DeleteFromDisk -Confirm:$false -RunAsync | Out-Null}\n</code></pre> <p>You can separate this script to just shutdown or just delete from disk. Use it whatever way you please. It\u2019s a simple script, but as always, if I used it at least a few times, it worth sharing \ud83d\ude42</p>"},{"location":"PowerShell/Bulk%20Add%20Users%20To%20Distribution%20Group/","title":"Bulk Add Users To Distribution Group","text":"<p>If you need to add multiple users to a distribution group in Office 365, you\u2019ll find out using the GUI can take a lot of time. Luckily, it only takes a CSV file and one line of code to do this automatically.</p> <p>First, save a CSV file with the email addresses you need, and make sure the first line is <code>displayname</code>.</p> <p></p> <p>First, test if PowerShell reads your CSV correctly as so:</p> <p></p> <p>If your output looks like the image above, then you\u2019re good to go. The command you need to run to add the users to the distribution list is:</p> <pre><code>Import-Csv $Location | foreach {Add-DistributionGroupMember -Identity \"Name of DL\" -Member $_.displayname}\n</code></pre> <p>For <code>$Location</code>, just type the path to the <code>.csv</code> file. It doesn\u2019t have to be a variable of course. For the <code>-Identity</code>, use the name of the group you want to add the users to.</p> <p>That\u2019s it. Hope this will be useful to someone \ud83d\ude42</p>"},{"location":"PowerShell/Bulk%20Set%20New%20IPs/","title":"Bulk Set New IPs","text":"<p>This script reaches out to each computer you have in your .csv and sets a static IP for it. Will you ever use it? Probably not because luckily, DHCP is a thing. I personally found it useful for a few very specific scenarios so here it is anyway \ud83d\ude42</p> <p>First, create a <code>.csv</code> file that looks like this:</p> Note <p>you can use different names for the headers, just make sure to change them in the script as well</p> <p></p> <p>Make sure you use \u201cName\u201d and \u201cNew IP\u201d as seen in the pictures.</p> <p>Info</p> <pre><code>Don't forget to change the `$GW` variable to your GW's IP.\n</code></pre> <pre><code>$vcenter = Read-Host (\"Enter VCenter:\")\nConnect-VIServer $vcenter\n$IPs = Import-Csv \"C:\\temp\\newIPs.csv\"\n$GW = \"GW\" # Change this to Gateway of your choosing!\nforeach ($item in $IPs)\n{\n    $hostname = $item.(\"Name\")\n    $New_IP = $item.(\"New IP\")\n    Invoke-Command -ComputerName $hostname -ScriptBlock {New-NetIPAddress -IPAddress $using:New_IP -DefaultGateway $using:GW -PrefixLength 24 -InterfaceIndex (Get-NetAdapter).InterfaceIndex -AsJob}\n    Get-Job | Stop-Job\n}\n</code></pre> <p>The script will set the static IPs of your choosing to your VMs. </p> <p>Enjoy \ud83d\ude0a</p> Notes <ul> <li>You will need to enable WinRM</li> <li>Don't forget to change the path of the script</li> </ul>"},{"location":"PowerShell/VMware%20Consolidation%20Alerts/","title":"VMware Consolidation Alerts","text":"<p>So you might have a few alerts already set thanks to a lot of public tools out there, but if you want to get an alert about a VM that needs consolidation without installing or buying anything, you can use a simple and short PowerShell script.</p> <pre><code>Connect-VIServer -Server \"your-vcenter\"\n$cneeded = Get-VM | Where-Object {$_.Extensiondata.Runtime.ConsolidationNeeded}\nif (!$null -eq $cneeded) {\n    Send-MailMessage -To \"your.email\" -From \"from@email.com\"  -Subject \"Consolidation Needed!\" -Body \"$cneeded\" -SmtpServer \"server-here\" -Port 25\n}\n</code></pre> <p>That\u2019s it. You can add it to a Task Scheduler and set the checks to happen whenever you like. Hope this was somewhat helpful \ud83d\ude42</p>"},{"location":"Projects/TwentyTwenty/","title":"TwentyTwenty","text":"<p>TwentyTwenty is a simple PyQt app that I wrote.</p> <p>This app is based on the 20-20-20 rule which you can read about in the links below. Basically, the rule says that every 20 minutes, look at something 20 feet away for 20 seconds. This will help reduce:</p> <ul> <li>sore, tired or burning eyes</li> <li>blurred or double vision</li> <li>watery, itchy or dry eyes</li> <li>headaches</li> </ul> <p>The app runs in the background and notifies you with a Windows 10 Toast + Sound notificaiton when it\u2019s time to look away and look back.</p> <p> </p> <p>By no means should you consider this app as an alternative to a medical exam. If you feel any pain or discomfort while working on your computer, you should consult a medical professional. You can read more about the 20-20-20 rule here:</p> <ul> <li>https://www.juststand.org/blog/prevent-eye-strain-with-the-20-20-20-rule/</li> <li>https://www.medicalnewstoday.com/articles/321536</li> <li>https://www.healthline.com/health/eye-health/20-20-20-rule</li> </ul>"},{"location":"Projects/TwentyTwenty/#download","title":"Download","text":"<p>Github</p>"},{"location":"Projects/VM%20Cloner/","title":"VM Cloner","text":"<p>VM Cloner is just a hobby project I wrote to allow myself to clone VMs in our vSphere environment.</p> <p>This script is pretty customized to my needs. Since every work environment is different, I'd make sure to read everything below and figure out if it suits you. This is my very first GUI project for Powershell. I can tell in advance that it's not perfect, but it's working. Any feedback is always welcome. Use this with caution, as always, your millage may vary.</p> <p>Github Link</p>"},{"location":"Prometheus/Alertmanager%20-%20Setup%20Beautiful%20Slack%20Alerts/","title":"Setup Templated Slack alerts with Alertmanager","text":"<p>When you configure Alertmanager to send you slack messages about your rules, it usually looks something like this: </p> <p>Let's turn it into a nicer looking message, the end result should look something like this: </p>"},{"location":"Prometheus/Alertmanager%20-%20Setup%20Beautiful%20Slack%20Alerts/#setup","title":"Setup","text":"<p>I'll be using KinD to setup a local K8s cluster and Helm to install kube-prometheus-stack. The community stack already includes all of the components we need for Prometheus like Alertmanager, Grafana, etc.</p> <p>I'll be creating the following files in a new folder I called <code>prometheus</code>: <pre><code>.\n\u251c\u2500\u2500 0-provider.tf\n\u251c\u2500\u2500 1-prometheus.tf\n\u2514\u2500\u2500 values\n    \u2514\u2500\u2500 values.yaml\n</code></pre></p> <ol> <li>Run <code>kind create cluster --name playground</code></li> <li>Create the Prometheus folder.</li> <li>Create the <code>0-provider.tf</code> file and let's configure Helm as our provider:    <pre><code>provider \"helm\" {\n  kubernetes {\n    config_path = \"~/.kube/config\"\n  }\n}\n</code></pre></li> <li>Now create <code>1-prometheus.tf</code> and configure our helm values:    <pre><code>resource \"helm_release\" \"prom-stack\" {\n  name = \"prom-stack\"\n\n  repository       = \"https://prometheus-community.github.io/helm-charts\"\n  chart            = \"kube-prometheus-stack\"\n  namespace        = \"prometheus\"\n  create_namespace = true\n\n  values = [file(\"values/values.yaml\")]\n}\n</code></pre></li> </ol> <p>Tip</p> <p>You can also avoid using Terraform by simply installing the chart manually:   <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\nhelm install [RELEASE_NAME] prometheus-community/kube-prometheus-stack\n</code></pre></p> <ol> <li>Get the chart's values and save them under <code>values/values.yaml</code>: <pre><code>helm show values prometheus-community/kube-prometheus-stack &gt; values/values.yaml\n</code></pre></li> <li>Run <code>terraform init</code> and then <code>terraform apply</code>. You should get a bunch of new pods, deployments, services: </li> </ol>"},{"location":"Prometheus/Alertmanager%20-%20Setup%20Beautiful%20Slack%20Alerts/#configuring-rules","title":"Configuring Rules","text":"<p>Now that we have Prometheus installed, let's configure some rules. I went with simple alerts for memory and CPU usage based on the resource limits.</p> <p>Create <code>rules.yaml</code> and paste in the following manifest: <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  labels:\n    role: alert-rules\n  name: custom-rules\n  namespace: prometheus\nspec:\n  groups:\n  - name: General Applicable Alerts\n    rules:\n    - alert: high_cpu_usage_resource_limits\n      expr: &gt;\n        (\n          sum by (container, pod, namespace) (rate(container_cpu_usage_seconds_total[1m]))\n        ) \n        / \n        (\n          sum by (container, pod, namespace) (kube_pod_container_resource_limits{resource=\"cpu\"})\n        ) &gt; 0.9\n      for: 1m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"High CPU usage in {{$labels.pod}}\"\n        description: \"Container *{{$labels.container}}* in pod *{{$labels.pod}}* is consuming *{{ $value | humanizePercentage }}* of its CPU limit\"\n\n    - alert: high_memory_usage_resource_limits\n      expr: &gt;\n        (\n          sum by (container, pod, namespace) (container_memory_usage_bytes)\n        ) \n        / \n        (\n          sum by (container, pod, namespace) (kube_pod_container_resource_limits{resource=\"memory\"})\n        ) &gt; 0.9\n      for: 1m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"High memory usage in {{$labels.pod}}\"\n        description: \"Container *{{$labels.container}}* in pod *{{$labels.pod}}* is consuming *{{ $value | humanizePercentage }}* of its memory limit\"\n</code></pre></p> <p>We're using the <code>kube_pod_container_resource_limits</code> metric to fetch the resource limits and the other metrics are calculating the CPU and memory usage on the pods.</p>"},{"location":"Prometheus/Alertmanager%20-%20Setup%20Beautiful%20Slack%20Alerts/#testing-the-metrics","title":"Testing The Metrics","text":"<p>Let's port-forward Prometheus and query those metrics. You can run <code>kubectl get svc -n &lt;namespace&gt;</code> to fetch the services you need: </p> <p>Now let's port-forward Prometheus: <code>kubectl port-forward svc/prom-stack-kube-prometheus-prometheus 9090:9090 -n prometheus</code></p> <p>You should be able to access Prometheus on <code>http://localhost:9090</code> and run the metric we used: <code>kube_pod_container_resource_limits</code>  The above returns all of our pods that this metric exposes. You can filter the <code>resource</code> to only get CPU or Memory results. It's also nice to see Prometheus helping with auto-completion: </p> <p>Let's run one of our rules to see what we get: <pre><code>(\n  sum by (container, pod, namespace) (container_memory_usage_bytes)\n) \n/ \n(\n  sum by (container, pod, namespace) (kube_pod_container_resource_limits{resource=\"memory\"})\n)\n</code></pre> The result is the ratio of memory used based on the resource's limits:  We can multiply by 100 to get the percentage, but we will handle that in the templating later.</p>"},{"location":"Prometheus/Alertmanager%20-%20Setup%20Beautiful%20Slack%20Alerts/#send-alerts","title":"Send Alerts","text":"<p>Now that we have our rules setup and ready, let's setup alerts for our Slack Channel. Go ahead and create a webhook for your Slack channel. It takes a couple of minutes.</p> <ol> <li>Go to the <code>values.yaml</code> file we created for the Prometheus stack and under <code>alertmanager.config</code>, you should be able to find <code>route</code>. Paste in the following config:    <pre><code>route:\n  group_by: ['alertname', 'container', 'pod', 'namespace']\n  group_wait: 30s\n  group_interval: 1m\n  repeat_interval: 3h\n  receiver: 'slack-notifications'\n  routes:\n  - receiver: 'slack-notifications'\n    matchers:\n      - alertname = \"Watchdog\"\nreceivers:\n- name: 'null'\n- name: 'slack-notifications'\n  slack_configs:\n  - send_resolved: false\n    text: \"{{ range .Alerts }}{{ .Annotations.description }}\\n{{ end }}\"\n    api_url: 'your-slack-webhook'\ntemplates:\n- '/etc/alertmanager/config/*.tmpl'\n</code></pre></li> <li>Run <code>tf apply</code> to apply the new values.</li> <li>Open your <code>rules.yaml</code> file and change the threshold (<code>0.9</code>) to <code>0.1</code>.</li> <li>Run <code>kubectl apply -f rules.yaml -n prometheus</code> to apply the <code>PrometheusRule</code> manifest to your cluster.</li> <li>You can verify those rules applied by checking the Prometheus UI:    </li> <li>Wait for a trigger, within a few minutes you should get your first alert, assuming one of your pods is using more than 10% of it's memory limits. You can always lower the threshold if you want.    </li> </ol>"},{"location":"Prometheus/Alertmanager%20-%20Setup%20Beautiful%20Slack%20Alerts/#test-cpu-rules","title":"Test CPU Rules","text":"<p>This part is optional, but you can test a CPU using a tool like stress.</p> <ol> <li>Create a Pod with resource limits:    <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: cpu-stress\nspec:\n  containers:\n  - name: cpu-stress\n    image: ubuntu\n    command: [\"/bin/bash\", \"-c\", \"--\"]\n    args: [\"while true; do sleep 30; done;\"]\n    resources:\n      requests:\n        cpu: \"500m\"\n        memory: \"256Mi\"\n      limits:\n        cpu: \"1000m\"\n        memory: \"500Mi\"\n</code></pre></li> <li>Exec into it: <code>kubectl exec -it cpu-stress -- bash</code></li> <li>Run <code>apt-get update &amp;&amp; apt-get install -y stress</code></li> <li>Run <code>stress --cpu 1</code>. A single core will be enough to trigger the alert, as we only allow one core as the limit of the pod anyway.</li> </ol>"},{"location":"Prometheus/Alertmanager%20-%20Setup%20Beautiful%20Slack%20Alerts/#templating-alerts","title":"Templating Alerts","text":"<p>We'll be using Monzo's Alertmanager Slack templates as our Slack templates. In another method of deploying Alertmanager, you would be able to create the template manually inside of the alertmanager pod. In our case, we'll add this template inside of our <code>values.yaml</code> file. It might not be the cleanest method, but for our demo this should do just fine :)</p> <ol> <li> <p>Under the same place we edited earlier (<code>alertmanager.config</code>), you can paste in the following: <pre><code>    route:\n      group_by: ['alertname', 'container', 'pod', 'namespace']\n      group_wait: 30s\n      group_interval: 1m\n      repeat_interval: 4h\n      receiver: 'slack-notifications'\n      routes:\n      - receiver: 'slack-notifications'\n        matchers:\n          - alertname = \"Watchdog\"\n    receivers:\n    - name: 'null'\n    - name: 'slack-notifications'\n      slack_configs:\n      - send_resolved: false\n        api_url: 'Slack-webhook-URL'\n        title: '{{ template \"slack.monzo.title\" . }}'\n        icon_emoji: '{{ template \"slack.monzo.icon_emoji\" . }}'\n        color: '{{ template \"slack.monzo.color\" . }}'\n        text: '{{ template \"slack.monzo.text\" . }}'\n        actions:\n          - type: button\n            text: \"Runbook :green_book:\"\n            url: \"{{ (index .Alerts 0).Annotations.runbook }}\"\n          - type: button\n            text: \"Query :mag:\"\n            url: '{{ template \"__generator_url_link\" . }}'\n          - type: button\n            text: \"Logs :grafana:\"\n            url: \"{{ (index .Alerts 0).Annotations.logs }}\"\n          - type: button\n            text: \"Silence :no_bell:\"\n            url: '{{ template \"__alert_silence_link\" . }}'\n          - type: button\n            text: '{{ template \"slack.monzo.link_button_text\" . }}'\n            url: \"{{ .CommonAnnotations.link_url }}\"\n    # templates:\n    # - '/etc/alertmanager/config/*.tmpl'\n    templates: [\"/etc/alertmanager/config/template_1.tmpl\"]\n  templateFiles:\n    template_1.tmpl: |-\n      {{ define \"__generator_url_link\" -}}\n          https://thanos.${base_domain}/alerts\n      {{- end }}\n      {{ define \"__alert_silence_link\" -}}\n          {{ .ExternalURL }}/#/silences/new?filter=%7B\n          {{- range .CommonLabels.SortedPairs -}}\n              {{- if ne .Name \"alertname\" -}}\n                  {{- .Name }}%3D\"{{- .Value -}}\"%2C%20\n              {{- end -}}\n          {{- end -}}\n          alertname%3D\"{{ .CommonLabels.alertname }}\"%7D\n      {{- end }}\n      {{ define \"__alert_severity_prefix\" -}}\n          {{ if ne .Status \"firing\" -}}\n          :warning:\n          {{- else if eq .Labels.severity \"critical\" -}}\n          :this-is-fine-fire:\n          {{- else if eq .Labels.severity \"warning\" -}}\n          :warning:\n          {{- else -}}\n          :curse-you-he-man:\n          {{- end }}\n      {{- end }}\n      {{ define \"__alert_severity_prefix_title\" -}}\n          {{ if ne .Status \"firing\" -}}\n          :warning:\n          {{- else if eq .CommonLabels.severity \"critical\" -}}\n          :this-is-fine-fire:\n          {{- else if eq .CommonLabels.severity \"warning\" -}}\n          :warning:\n          {{- else if eq .CommonLabels.severity \"info\" -}}\n          :information_source:\n          {{- else -}}\n          :curse-you-he-man:\n          {{- end }}\n      {{- end }}\n      {{/* First line of Slack alerts */}}\n      {{ define \"slack.monzo.title\" -}}\n          [{{ .Status | toUpper -}}\n          {{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{- end -}}\n          ] {{ template \"__alert_severity_prefix_title\" . }} {{ .CommonLabels.alertname }}\n      {{- end }}\n      {{/* Color of Slack attachment (appears as line next to alert )*/}}\n      {{ define \"slack.monzo.color\" -}}\n          {{ if eq .Status \"firing\" -}}\n              {{ if eq .CommonLabels.severity \"warning\" -}}\n                  warning\n              {{- else if eq .CommonLabels.severity \"critical\" -}}\n                  danger\n              {{- else -}}\n                  #439FE0\n              {{- end -}}\n          {{ else -}}\n          good\n          {{- end }}\n      {{- end }}\n      {{/* Emoji to display as user icon (custom emoji supported!) */}}\n      {{ define \"slack.monzo.icon_emoji\" }}:prometheus:{{ end }}\n      {{/* The test to display in the alert */}}\n      {{ define \"slack.monzo.text\" -}}\n      {{ range .Alerts }}\n      {{- if .Annotations.message }}\n      {{ .Annotations.message }}\n      {{- end }}\n      {{- if .Annotations.description }}\n      {{ .Annotations.description }}\n      {{- end }}\n      {{- end }}\n      {{- end }}\n      {{- /* If none of the below matches, send to #monitoring-no-owner, and we \n      can then assign the expected code_owner to the alert or map the code_owner\n      to the correct channel */ -}}\n      {{ define \"__get_channel_for_code_owner\" -}}\n          {{- if eq . \"platform-team\" -}}\n              platform-alerts\n          {{- else if eq . \"security-team\" -}}\n              security-alerts\n          {{- else -}}\n              monitoring-no-owner\n          {{- end -}}\n      {{- end }}\n      {{- /* Select the channel based on the code_owner. We only expect to get\n      into this template function if the code_owners label is present on an alert.\n      This is to defend against us accidentally breaking the routing logic. */ -}}\n      {{ define \"slack.monzo.code_owner_channel\" -}}\n          {{- if .CommonLabels.code_owner }}\n              {{ template \"__get_channel_for_code_owner\" .CommonLabels.code_owner }}\n          {{- else -}}\n              monitoring\n          {{- end }}\n      {{- end }}\n      {{ define \"slack.monzo.link_button_text\" -}}\n          {{- if .CommonAnnotations.link_text -}}\n              {{- .CommonAnnotations.link_text -}}\n          {{- else -}}\n              Link\n          {{- end }} :link:\n      {{- end }}\n</code></pre>    This is more or less Monzo's template with a few slight changes.</p> </li> <li> <p>run <code>tf apply</code> to apply the changes to our stack.</p> </li> <li>The <code>templateFiles</code> attribute is loaded a secret to Alertmanager. You can verify the new template <code>template_1.yml</code> applied by running: <code>kubectl get secret -o yaml alertmanager-prom-stack-kube-prometheus-alertmanager</code>:    <pre><code>apiVersion: v1\ndata:\n  alertmanager.yaml: Z2xvYmFsOgogIHJlc29sdsdfRTd1.....==\n  template_1.tmpl: e3sgZGVmaW5lICJfX2dlbmVyYXRvcl.....==\nkind: Secret\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: prom-stack\n    meta.helm.sh/release-namespace: prometheus\n  creationTimestamp: \"2024-01-29T11:33:46Z\"\n  labels:\n    app: kube-prometheus-stack-alertmanager\n    app.kubernetes.io/instance: prom-stack\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    app.kubernetes.io/version: 56.2.1\n    chart: kube-prometheus-stack-56.2.1\n    heritage: Helm\n    release: prom-stack\n  name: alertmanager-prom-stack-kube-prometheus-alertmanager\n  namespace: prometheus\n  resourceVersion: \"1218773\"\ntype: Opaque\n</code></pre>    To save us a wall of text, I truncated the base64 output that you would see under <code>template_1.tmpl</code>.</li> <li>Without any changes to our rules, our new alert should look like this: </li> </ol> <p>Tip</p> <p>If you're having issues with getting alerts, you can run <code>kubectl logs -f alertmanager-prom-stack-kube-prometheus-alertmanager</code> to see the logs of alertmanager.</p>"},{"location":"Prometheus/Alertmanager%20-%20Setup%20Beautiful%20Slack%20Alerts/#how-do-i-get-that-grafana-link","title":"How do I get that Grafana link?","text":"<p>Optionally, you can add a Grafana botton by adding some of the Annotations that we get from the template. In my case, I added the <code>logs</code> label to my rule: <pre><code> - alert: high_memory_usage_resource_limits\n   expr: &gt;\n     (\n       sum by (container, pod, namespace) (container_memory_usage_bytes)\n     ) \n     / \n     (\n       sum by (container, pod, namespace) (kube_pod_container_resource_limits{resource=\"memory\"})\n     ) &gt; 0.9\n   for: 1m\n   labels:\n     severity: warning\n   annotations:\n     summary: \"High memory usage in {{$labels.pod}}\"\n     description: \"Container *{{$labels.container}}* in pod *{{$labels.pod}}* is consuming *{{ $value | humanizePercentage }}* of its memory limit\"\n     logs: http://localhost:8080/d/k8s_views_pods/kubernetes-views-pods?orgId=1&amp;refresh=30s&amp;var-datasource=prometheus&amp;var-cluster=&amp;var-namespace={{$labels.namespace}}&amp;var-pod={{$labels.pod}}&amp;var-resolution=1m&amp;var-job=kube-state-metrics\n</code></pre></p> <p>By default, the Monzo annotation refers to <code>dashboard</code> as the label, but I edited it to work as <code>logs</code>. Under this label you can post your Grafana URL with the labels from the query, which would bring you exactly to the Pod's stats (assuming your dashboard supports it).</p> <p>That's it!</p>"},{"location":"Python/GitHub%20Pagination/","title":"GitHub Pagination","text":"<p>We recently needed to fetch a list of all of our repos and their branches for an internal tool we're working on and I wanted to document how we handled pagination.</p>"},{"location":"Python/GitHub%20Pagination/#prepare","title":"Prepare","text":"<p>Create a <code>.env</code> file in your directory with:</p> <ul> <li>Your PAT.</li> <li>Organization Name</li> </ul>"},{"location":"Python/GitHub%20Pagination/#usage","title":"Usage","text":"<p>Let's start with the basics: <pre><code>import os\nimport requests\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ngithub_token = os.getenv('GITHUB_TOKEN')\norganization = os.getenv('GITHUB_ORGANIZATION')\nrepo_name = \"your-repo-name\"\n</code></pre></p> <p>We can now make our GET request: <pre><code>url = f\"https://api.github.com/repos/{organization}/{repo_name}/branches?per_page=10\"\nheaders = {'Authorization': f'token {github_token}',\n            'Accept': \"application/vnd.github+json\"}\nres = requests.get(url, headers=headers)\n</code></pre></p> <p><code>res</code> will print out a list of repos: <pre><code>[{'name': '39d1f185c83a44538781',\n  'commit': {'sha': 'commit-sha',\n   'url': 'https://api.github.com/repos/org/repo/commits/commit-sha'},\n   ...\n   ...\n</code></pre></p> <p>Next, we can see that <code>res.links</code> fetches the following answer: <pre><code>{'next': {'url': 'https://api.github.com/repositories/repo/branches?per_page=10&amp;page=2',\n  'rel': 'next'},\n 'last': {'url': 'https://api.github.com/repositories/repo/branches?per_page=10&amp;page=9',\n  'rel': 'last'}}\n</code></pre></p> <p>We loop over <code>res.links</code> while <code>next</code> exists and fill the <code>branches</code> list until there are no more pages: <pre><code>branches = []\nres = requests.get(url, headers=headers)\nbranches.extend([branch['name'] for branch in res.json()])\n\nwhile 'next' in res.links.keys():\n    res = requests.get(res.links['next']['url'], headers=headers)\n    branches.extend([branch['name'] for branch in res.json()])\n</code></pre></p> <p>That's pretty much it.</p>"},{"location":"Python/GitHub%20Pagination/#notes","title":"Notes","text":"<p>A little after writing this document we started using GraphQL to query the repos and branches. It's a lot faster and uses less requests to Github's endpoint. I'll cover that in the future :)</p>"},{"location":"Python/Python%20Pandas%20Example/","title":"Python Pandas Example","text":"<p>I automated a silly task as a beginner\u2019s project in Python. It was a nice learning experience as I don\u2019t do this quite often.</p> <p>So we have a list of upcoming new employees in an Excel file. It has their name and start date. There are a few people in our office that need a reminder that new people are joining, so this script sends them that email with all the details.</p> <p>Generally, if there\u2019s a new employee on that list that starts in less than 10 days, it adds him to a table and sends that in an email. I run this script in a cron-job once a week at the same hour.</p> <pre><code>import pandas as pd\nimport datetime as dt\nimport smtplib, ssl\nfrom email.mime.multipart import MIMEMultipart\n\ndef sendEmail():\n    sender_email = \"\"\n    receiver_email = \"\"\n\n    message = MIMEMultipart(\"alternative\")\n    message[\"Subject\"] = \"New Employees Reminder\"\n    message[\"From\"] = sender_email\n    message[\"To\"] = receiver_email\n\n    # Create the plain-text and HTML version of your message\n    text = \"\"\"\n    Test\n    \"\"\"\n    #Open the HTML file we created because we don't want to output raw HTML code. This adds the new employees table to the email.\n    with open('employees.html') as fp:\n        html = fp.read()\n\n    part1 = MIMEText(text, \"plain\")\n    part2 = MIMEText(html, \"html\")\n    message.attach(part1)\n    message.attach(part2)\n\n    try:\n        smtpObj = smtplib.SMTP('server')\n        smtpObj.sendmail(sender_email, receiver_email, message.as_string())         \n        print (\"Successfully sent email\")\n    except SMTPException:\n        print (\"Error: unable to send email\")\n\n#Read the Excel file, read the specific sheet we need and take just the column we're interested in: Start Date\nxls = pd.ExcelFile(r'testfile.xlsx')\ndf = pd.read_excel(xls, 'New Employment')\ndf['Start Date'] = pd.to_datetime(df['Start Date'])\ntoday = pd.Timestamp.today()\n\n#Calculate how many days are left til the employee starts working using Pandas. It's a delta from today's date and the date in the Excel file.\ndf['Days Until Start'] = ((df['Start Date'] - today).dt.days)\ndf['Days Until Start'] = df['Days Until Start'].astype(int)\ndelta_df = df[['Name', 'Days Until Start']]\nclose_delta = delta_df[delta_df['Days Until Start'].between(0,10)] #Make sure the employees delta is no bigger than 0 because that means he already started working and smaller than 10 (reasonable time to prepare for a new employee)\nclose_delta.to_html('employees.html') #Output the data into a table inside the HTML file which we will use for the \"Send Email\" function.\n\nif len(close_delta) &gt; 0: #Check if there are any employees queued up, so it won't send an email with an empty list in case there are no new employees soon.\n    sendEmail()\n</code></pre> <p>The output looks something like this:</p> <p></p> <p>Hope this will be helpful to someone \ud83d\ude0a</p>"},{"location":"SCCM/Applying%20Drivers%20The%20Easy%20Way/","title":"Applying Drivers The Easy Way","text":"<p>There is the traditional way of importing drivers where you download driver packs, import the drivers and then create a package for them.</p> <p>I decided to go with a less known, but better way of applying drivers to your OSD.</p> <p>First, we\u2019ll download the drivers we want to apply. For this example, I will use drivers for HP 840 G3.</p> Drivers for HP <p>Each vendor has driver packs for his computers. In my case, this is HP\u2019s website.</p> <p>I\u2019ll find the latest drivers and download them.</p> <p></p> <p>I\u2019ll download and transfer the file to a folder I created inside the SCCM server. Open the .exe file and extract the files to our folder.</p> <p></p> <p>Now, go to <code>Software Library &gt; Packages</code> and create a new regular package.</p> <p> </p> <p>Now that the package is created, distribute it to our DP</p> <p> </p> <p>Now that we have the package, we\u2019ll go to our TS and edit it.</p> <p>We\u2019ll go to the drivers section and create a new \u201ccommand line\u201d step.</p> <p></p> <p>We\u2019ll add the following command as the command line: <pre><code>DISM.exe /Image:%OSDisk%\\ /Add-Driver /Driver:.\\ /Recurse\n</code></pre> And choose the package we created as package: </p> <p>In options, we\u2019ll create a new Query WMI and use the following statement: <pre><code>SELECT * FROM Win32_ComputerSystem WHERE Model = \"HP EliteBook 840 G3\"\n</code></pre></p> <p></p> <p>Testing this we can see it is valid:</p> <p></p> <p>Set up Success codes as this: <code>2 50</code></p> <p></p> <p>You can see it works during deployment:</p> <p></p> <p>That\u2019s it.</p> <p>To add new packages, for different computers, we\u2019ll do everything the same except we need to change the Query to match the model number.</p> Information <p>To get a model number of a computer, use the following command in CMD:  <pre><code>WMIC CSPRODUCT GET NAME\n</code></pre> Always use the full name.</p>"},{"location":"SCCM/Automatically%20Sort%20Computers%20Into%20Collections/","title":"Automatically Sort Computers Into Collections","text":"<p>If you\u2019re looking to organize new computers that you deploy, then this might be helpful.</p> <p>First, save the following code as <code>AddMeToCollection.vbs</code></p> <pre><code>'==========================================================================\n '\n ' NAME: AddMeToCollection.vbs\n ' \n ' AUTHOR: Vinay, Microsoft\n ' DATE  : 8/7/2010\n '\n ' COMMENT: Script to add Unknown Computer to a specified collection during OSD\n ' USAGE: cscript AddMeToCollection.vbs   %_SMSTSClientIdentity%\n '==========================================================================\n On Error Resume Next\n Dim arrArguments\n Set arrArguments = WScript.Arguments\n If arrArguments.Count &lt;&gt; 3 Then\n     WScript.Echo \"Usage: cscript AddMeToCollection.vbs   %_SMSTSClientIdentity%\"\n     WScript.Echo \" and  needs to be specified, but last parameter needs to be used as is.\"\n     WScript.Quit\n End If\n Dim strServer, strCollID, strProvNamespace\n Dim strComputerName, strGUID, strResourceID\n Dim strUser, strPassword\n strResourceID = 0\n strServer = arrArguments(0)\n strCollID = arrArguments(1)\n strGUID = arrArguments(2)\n WScript.Echo \"\"\n WScript.Echo \"===================================\"\n WScript.Echo \"   ADDING COMPUTER TO COLLECTION\"\n WScript.Echo \"===================================\"\n WScript.Echo \"Site Server specified: \" &amp; strServer\n WScript.Echo \"Collection ID specified: \" &amp; strCollID\n WScript.Echo \"SMS Client \" &amp; strGUID\n 'Get the computer name\n Set oNet = CreateObject(\"WScript.Network\")\n strComputerName = oNet.ComputerName\n WScript.Echo \"Computer Name: \" &amp; strComputerName\n Set oNet = Nothing\n 'Connect to root/sms namespace on SMS Site Server to find the Provider Namespace\n Set objLocator = CreateObject(\"WbemScripting.SWbemLocator\")    \n Set oWbem = objLocator.ConnectServer(strServer, \"root/sms\")  \n If Err.number &lt;&gt; 0 Then\n     WScript.Echo \"Error connecting to root\\sms namespace to find Provider Location. Exiting!\"\n     WScript.Echo \"Error = \" &amp; Err.number &amp; \" - \" &amp; Err.Description\n     WScript.Quit\n End If\n Set colNameSpace = oWbem.ExecQuery(\"SELECT * FROM SMS_ProviderLocation\")\n For Each item in colNameSpace\n        WScript.Echo \"SMS Provider Namespace = \" &amp; item.NamespacePath\n     strProvNamespace = item.NamespacePath\n Next\n 'Connect to the Provider Namespace\n Set oWbem = objLocator.ConnectServer(strServer, strProvNamespace)\n If Err.number &lt;&gt; 0 Then\n     WScript.Echo \"Error connecting to SMS Provider namespace. Exiting!\"\n     WScript.Echo \"Error = \" &amp; Err.number &amp; \" - \" &amp; Err.Description\n     WScript.Quit\n Else\n     WScript.Echo \"Successfully Connected to the SMS Provider Namespace\"\n End If\n 'Find out the Resource ID of the computer by querying SMS_R_System Class against the SMS GUID\n Set colResources = oWbem.ExecQuery(\"SELECT ResourceID FROM SMS_R_System WHERE SMSUniqueIdentifier = '\" &amp; strGUID &amp; \"'\")\n For Each oResource In colResources\n     strResourceID = oResource.ResourceID\n     WScript.Echo \"Resource ID = \" &amp; strResourceID\n Next\n 'If Resource ID was not found, exit gracefully\n If strResourceID = 0 Then\n     WScript.Echo \"Could not find the Resource ID for the computer. Exiting!\"\n     WScript.Quit\n End If\n 'Verify if the specified collection exists\n Set oCollection = oWbem.Get(\"SMS_Collection.CollectionID=\" &amp; \"\"\"\" &amp; strCollID &amp; \"\"\"\")\n If oCollection.Name = \"\" Then\n     WScript.Echo \"Specified Collection (\" &amp; strCollID &amp; \") was Not Found. Exiting!\"\n     WScript.Quit\n End If\n 'Create a Direct Membership rule\n Set oDirectRule = oWbem.Get(\"SMS_CollectionRuleDirect\").SpawnInstance_ ()\n oDirectRule.ResourceClassName = \"SMS_R_System\"\n oDirectRule.ResourceID = strResourceID\n oDirectRule.RuleName = strComputerName &amp; \" - SMSTS\"\n 'Add the Direct Membership Rule to the specified collection\n oCollection.AddMembershipRule oDirectRule\n If Err.Number &lt;&gt; 0 Then\n     WScript.Echo \"Could not add the computer to the specified collection. Exiting!\"\n     WScript.Echo \"Error = \" &amp; Err.number &amp; \" - \" &amp; Err.Description\n     WScript.Quit\n Else\n     WScript.Echo strComputerName &amp; \" successfully added To \" &amp; strCollID\n End If\n WScript.Echo \"===================================\"\n WScript.Echo \"\"\n Set objLocator = Nothing\n Set oWbem = Nothing\n Set oCollection = Nothing\n Set oDirectRule = Nothing\n 'End Script\n</code></pre> <p>Move it to a folder in your SCCM server and create a new package.</p> <p></p> <p></p> <p>Now go to your Task Sequence and add a command line</p> <p></p> <p>Add the following command: <pre><code>cscript AddMeToCollection.vbs sccm.server.com XX100028 %_SMSTSClientIdentity%\n</code></pre></p> <ul> <li>Where <code>sccm.server.com</code> is, replace it with your SCCM server.</li> <li>For <code>XX1000E1</code>, replace it with the collection\u2019s ID.</li> </ul> <p></p> <p>I personally had to use a privileged account for it to work. I apply this after the client is installed, and from my testing, it failed without admin rights. As always, your mileage may vary.</p>"},{"location":"SCCM/Automatically%20Sort%20Computers%20Into%20Collections/#how-it-can-be-used","title":"How it can be used","text":"<p>This script can be super useful in environments where you deploy all kinds of image versions for different departments. For example, I use TsGui and created an XML GUI to choose which customization to apply to the image using variables. This script helped me add the device I\u2019m imaging to the correct collection because I configure each department separately, as can be seen here:</p> <p></p> <p></p> Note <p>In my environment, the new device appears as \u201cUnknown\u201d fora while (15-30 minutes) but eventually gets the correct hostname.</p> <p>Hope this has been useful to you \ud83d\ude42</p>"},{"location":"SCCM/Deploy%20Visual%20Studio%20with%20SCCM/","title":"Deploy Visual Studio with SCCM","text":"<p>In this tutorial, we\u2019ll go through deploying VS17 in SCCM.</p> <p>Download the installer and save it under a folder of your choice. For this tutorial I\u2019ll use <code>C:\\VS17</code></p> <p></p> <p>Open CMD and use -layout command of your choice. My command: <pre><code>vs_enterprise.exe --layout c:\\vslayout --add\nMicrosoft.VisualStudio.Workload.ManagedDesktop --add\nMicrosoft.VisualStudio.Workload.NetWeb --add\nComponent.GitHub.VisualStudio --includeOptional --lang en-US\n</code></pre></p> <p>More VS Components</p> <p>Add components as you please, the list can be found here.</p> <p>Note</p> <p>the full featured installation requires about 40GB of free space. </p> <p></p> <p>It will now download all the necessary files to the folder.</p> <p></p> <p>Tip</p> <p>Grab some coffee, it can take a while</p> <p></p> <p></p> <p>Copy all the files to your SCCM server and let\u2019s create a new App to deploy.</p> <p></p> <p></p> <p></p> <p></p> <p>Point to the offline files we downloaded earlier as the \u201ccontent location\u201d.</p> <p>For the installation command we\u2019ll use the same command we used for the <code>\u2013layout</code> command earlier when we downloaded the files offline. Just add your desired switches to the installation.</p> <p>Note</p> <ul> <li>Add <code>\u2013productKey</code> to use your license</li> <li>More information on switches.</li> </ul> <pre><code>Installation Program: vs_enterprise.exe --add\nMicrosoft.VisualStudio.Workload.ManagedDesktop --add\nMicrosoft.VisualStudio.Workload.NetWeb --add\nComponent.GitHub.VisualStudio --includeOptional --quiet --wait --norestart --productKey XXXXXXXXXXXXXXXXXXXX\n</code></pre> <p>Tip</p> <p>Uninstall: <code>vs_enterprise.exe --uninstall</code></p> <p></p> <p>I personally only use file detection as the discovery method. Use the path to the default installation folder and search for <code>devenv.exe</code></p> <p>Path: <code>%ProgramFiles(x86)%\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE</code></p> <p></p> <p></p> <p>That\u2019s it. Deploy and test to see if it works.</p> Tip <ul> <li>Don\u2019t forget to to distribute to your DP.</li> <li>You can improve this deployment by adding another detection method for the VS version you\u2019re installing.</li> <li>You can also add dependencies for this to be installed if you\u2019re deploying outside of your task sequence.</li> </ul> <p>Good luck \ud83d\ude42</p>"},{"location":"SCCM/Resize%20Client%20Cache/","title":"Resize Client Cache","text":"<p>Resizing or clearing cache can be useful for many things. In this tutorial I\u2019ll show you how I do it during my Task Sequence. I personally needed it to install Visual Studio during OSD. The default size is 5120MB which is not enough in my case.</p>"},{"location":"SCCM/Resize%20Client%20Cache/#resize-cache-size","title":"Resize Cache Size","text":"<pre><code>$UIResourceMgr = New-Object -ComObject UIResource.UIResourceMgr\n $Cache = $UIResourceMgr.GetCacheInfo()\n $Cache.TotalSize = 25600\n</code></pre>"},{"location":"SCCM/Resize%20Client%20Cache/#clear-cache","title":"Clear Cache","text":"<pre><code>$UIResourceMgr = New-Object -ComObject UIResource.UIResourceMgr\n $Cache = $UIResourceMgr.GetCacheInfo()\n $CacheElements = $Cache.GetCacheElements()\n foreach ($Element in $CacheElements) {\n     $Cache.DeleteCacheElement($Element.CacheElementID)\n }\n</code></pre> <p>Save them all as .ps1 files and place in the same folder.</p> <p></p> <p>Create a package, without a program, and don\u2019t forget to distribute it to your DP.</p> <p></p> <p>Now you can use it as a regular PS1 script in your TS as so:</p> <p></p> <p>For my TS, I increase the cache size, install all the software I need, clear it and then decrease it back to the default setting, since I don\u2019t want it taking too much space on the hard drive.</p> <p>Hope this has been helpful in some way \ud83d\ude42</p>"},{"location":"SCCM/Set%20Power%20Plan%20with%20SCCM/","title":"Set Power Plan with SCCM","text":"<p>In this guide I\u2019ll show you how to set the Power Plan of Windows to High Performance during the OSD.</p> <p>This is good for one main reason: Speed up the deployment time. It seems that after some tests, this can speed up the OSD by 20-50% (depending on the environment and the deployment).</p> <p>First, create a new \u201cRun Command Line\u201d step.</p> <p>In the command line add this:</p> <pre><code>PowerCfg.exe /s 8c5e7fda-e8bf-4a96-9a85-a6e23a8c635c\n</code></pre> <p></p> Note <p>Every power plan has it\u2019s own GUID. If you wish to set it back to something else, simply choose your GUID and implement it later in the TS.</p> Mode Description GUID <code>Power Saver</code> Delivers reduced performance which may increase power savings. <code>a1841308-3541-4fab-bc81-f71556f20b4a</code> <code>Balanced</code> Automatically balances performance and power consumption according to demand. <code>381b4222-f694-41f0-9685-ff5bb260df2e</code> <code>High Performance</code> Delivers maximum performance at the expense of higher power consumption. <code>8c5e7fda-e8bf-4a96-9a85-a6e23a8c635c</code> <p>Under the Options tab, create a TS Variable like this:</p> <p></p> <p>Now, I also added this power plan to take affect during WinPE as well like this:</p> <pre><code>X:\\Windows\\System32\\PowerCfg.exe /s 8c5e7fda-e8bf-4a96-9a85-a6e23a8c635c\n</code></pre> <p></p> <p>For testing purposes, I created a TS for this alone:</p> <p></p> <p>Deploying it to my machine worked great:</p> <p></p> <p>That's it, hope it helps someone \ud83d\ude0a</p>"}]}